{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 문서를 보면\n",
    "\n",
    "이후에 튜토리얼에서는 코드를 보여줄 것이나, 좀 더 자세히 알고 싶다면 tensorflow/examples/tutorials/word2vec/word2vec_basic.py 의 최소화된 구현을 참고하자. 이 기본 예제는 특정 데이터를 다운로드 하기 위해 필요한 코드, 이것을 약간 학습하기 위한 코드, 그리고 결과를 시각화하기 위하 코드를 포함한다. 기본 버전을 읽고 실행하는데 익숙해지면, 쓰레드를 이용하여 어떻게 효율적으로 데이터를 텍스트 모델로 이동시키는지, 학습하는 동안 어떻게 체크하는지 등에 대한 좀 더 심화된 TensorFlow 원리들을 보여주는 심화 구현된 tensorflow_models/tutorials/embedding/word2vec.py 을 시작할 수 있다.\n",
    "\n",
    "\n",
    "그래서 위 예제를 먼저 시작했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모르는 것을 검색하며 보다가 좋은 자료를 많이 찾았는데\n",
    "\n",
    "> 좀 더 상세하고 자세히 코드리뷰를 한 http://pythonkim.tistory.com/93 블로그.\n",
    "\n",
    "> Word Embedding이나 기본적인 이론에서 관한 부분이 잘 정리된 기본 텐서플로우 문서.  \n",
    "> https://tensorflowkorea.gitbooks.io/tensorflow-kr/g3doc/tutorials/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5244, 3083, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3083 originated -> 5244 anarchism\n",
      "3083 originated -> 12 as\n",
      "12 as -> 3083 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Initialized\n",
      "Average loss at step  0 :  274.317626953\n",
      "Nearest to with: saito, moustache, webpages, pipelined, babbitt, occultism, theosophical, bryozoa,\n",
      "Nearest to which: widows, adnan, macroeconomics, cricetulus, ran, emphasises, effluents, alpher,\n",
      "Nearest to who: wronged, blocked, relena, legislative, vogue, lasker, doppelbock, diss,\n",
      "Nearest to at: brinkley, receiver, veracruz, carolingian, sputtering, discounted, kola, triune,\n",
      "Nearest to been: gallipoli, salon, quentin, widow, pricing, extremes, visionaries, adb,\n",
      "Nearest to world: resulting, supplant, chee, demolished, continuous, dining, sarawak, unbreakable,\n",
      "Nearest to five: idm, materialistic, swnt, cayman, boulevard, plasmid, boxes, quarreled,\n",
      "Nearest to over: lunches, dura, exec, moritz, handler, inlet, glossolalia, estonians,\n",
      "Nearest to also: indigo, wakefield, deepening, misery, fraud, ntfs, identity, logos,\n",
      "Nearest to are: loyal, illuminated, contrived, fairly, stair, valve, wilfrid, middle,\n",
      "Nearest to new: election, rushes, ibrd, stresemann, brightest, satisfied, kenner, lewin,\n",
      "Nearest to three: hampering, lio, histories, waterbury, corinthians, crichton, deregulated, mbta,\n",
      "Nearest to b: foe, vsl, really, vertebrae, ferus, weakest, liable, slaying,\n",
      "Nearest to first: sgt, murders, fault, biochemical, rollin, halliday, outputs, agreeable,\n",
      "Nearest to it: kerry, navigator, drawer, soa, settler, org, histone, prefixes,\n",
      "Nearest to called: quintus, paralleled, roommates, stored, evolutionarily, stripes, decomposes, reels,\n",
      "Average loss at step  2000 :  113.464368629\n",
      "Average loss at step  4000 :  52.2969048369\n",
      "Average loss at step  6000 :  33.4798051081\n",
      "Average loss at step  8000 :  23.5470400534\n",
      "Average loss at step  10000 :  17.5864846945\n",
      "Nearest to with: and, in, for, of, man, mya, theists, on,\n",
      "Nearest to which: this, basins, that, altenberg, UNK, tiny, completeness, macintosh,\n",
      "Nearest to who: legislative, and, blocked, de, mimicking, atoms, explosive, since,\n",
      "Nearest to at: in, of, omega, and, receiver, electric, continues, ashes,\n",
      "Nearest to been: finalist, gallipoli, difficult, customs, salon, extremes, nine, lnot,\n",
      "Nearest to world: resulting, continuous, fao, dining, influences, federal, demolished, task,\n",
      "Nearest to five: zero, nine, seven, three, one, dim, september, six,\n",
      "Nearest to over: lunches, mosque, malcolm, molinari, nine, zero, revolution, derleth,\n",
      "Nearest to also: lymphoma, antiwar, greenhouses, setting, examination, was, mac, rice,\n",
      "Nearest to are: is, were, loyal, toxic, flaw, sixteenth, fairly, defect,\n",
      "Nearest to new: election, panhandle, country, rushes, satisfied, vs, sustain, reinforced,\n",
      "Nearest to three: one, zero, arlington, nine, eight, five, two, mathbf,\n",
      "Nearest to b: a, foe, really, archie, occurs, existing, except, councils,\n",
      "Nearest to first: of, murders, techno, sandstone, in, carry, opinions, genres,\n",
      "Nearest to it: he, UNK, a, coke, settled, lymphoma, vs, and,\n",
      "Nearest to called: agave, wild, difference, greenhouses, meetings, quintus, stored, saudi,\n",
      "Average loss at step  12000 :  14.3139679366\n",
      "Average loss at step  14000 :  11.8675417757\n",
      "Average loss at step  16000 :  9.86302533329\n",
      "Average loss at step  18000 :  8.56352528822\n",
      "Average loss at step  20000 :  8.04505437231\n",
      "Nearest to with: for, and, in, of, by, on, dasyprocta, nine,\n",
      "Nearest to which: that, this, UNK, and, agouti, altenberg, circ, one,\n",
      "Nearest to who: and, legislative, circ, not, trapezohedron, de, no, diss,\n",
      "Nearest to at: in, and, of, omega, for, antwerp, with, three,\n",
      "Nearest to been: was, nine, by, customs, difficult, pricing, subkey, agouti,\n",
      "Nearest to world: resulting, continuous, dining, influences, demolished, fao, rifles, federal,\n",
      "Nearest to five: nine, zero, eight, six, three, seven, operatorname, four,\n",
      "Nearest to over: lunches, nine, yin, malcolm, circ, molinari, mosque, operatorname,\n",
      "Nearest to also: lymphoma, circ, continual, greenhouses, sparse, agouti, antiwar, was,\n",
      "Nearest to are: is, were, was, and, toxic, tree, loyal, had,\n",
      "Nearest to new: election, rushes, agouti, apatosaurus, panhandle, satisfied, dahomey, country,\n",
      "Nearest to three: eight, nine, two, five, one, operatorname, seven, six,\n",
      "Nearest to b: d, foe, circ, except, sx, liable, clem, existing,\n",
      "Nearest to first: of, numa, sandstone, carry, opinions, in, murders, techno,\n",
      "Nearest to it: he, there, this, coke, and, settled, the, agouti,\n",
      "Nearest to called: and, wattle, armistice, greenhouses, meetings, quintus, wild, described,\n",
      "Average loss at step  22000 :  6.96181236744\n",
      "Average loss at step  24000 :  6.76118254066\n",
      "Average loss at step  26000 :  6.80878613949\n",
      "Average loss at step  28000 :  6.38149649203\n",
      "Average loss at step  30000 :  5.91084523523\n",
      "Nearest to with: for, by, and, in, on, dasyprocta, from, at,\n",
      "Nearest to which: that, this, it, agouti, altenberg, but, and, dasyprocta,\n",
      "Nearest to who: and, circ, legislative, de, not, trapezohedron, no, chi,\n",
      "Nearest to at: in, and, for, on, with, three, by, four,\n",
      "Nearest to been: by, was, were, mishnayot, customs, primigenius, be, become,\n",
      "Nearest to world: continuous, resulting, dining, influences, enki, klan, dialectic, demolished,\n",
      "Nearest to five: eight, six, zero, four, seven, nine, three, operatorname,\n",
      "Nearest to over: six, lunches, yin, malcolm, circ, molinari, with, mosque,\n",
      "Nearest to also: often, it, not, circ, sparse, lymphoma, continual, agouti,\n",
      "Nearest to are: were, is, was, had, tree, have, by, flaw,\n",
      "Nearest to new: election, rushes, agouti, birkenau, satisfied, mlb, dahomey, apatosaurus,\n",
      "Nearest to three: two, five, eight, four, six, seven, nine, zero,\n",
      "Nearest to b: d, foe, circ, and, except, liable, sx, converts,\n",
      "Nearest to first: inlay, numa, canaris, carry, last, gollancz, biochemical, genres,\n",
      "Nearest to it: he, there, this, coke, they, which, not, agouti,\n",
      "Nearest to called: and, armistice, greenhouses, wattle, hobbyist, often, cured, overthrows,\n",
      "Average loss at step  32000 :  6.00206856632\n",
      "Average loss at step  34000 :  5.6912967397\n",
      "Average loss at step  36000 :  5.78366493499\n",
      "Average loss at step  38000 :  5.49219698679\n",
      "Average loss at step  40000 :  5.29227393281\n",
      "Nearest to with: for, in, and, by, on, zero, from, dasyprocta,\n",
      "Nearest to which: that, this, it, but, one, and, agouti, altenberg,\n",
      "Nearest to who: and, circ, he, which, trapezohedron, not, legislative, sometimes,\n",
      "Nearest to at: in, on, and, with, rang, for, average, zero,\n",
      "Nearest to been: was, were, by, be, become, customs, primigenius, reply,\n",
      "Nearest to world: continuous, resulting, dining, dialectic, influences, rifles, enki, demolished,\n",
      "Nearest to five: six, four, eight, three, seven, zero, nine, two,\n",
      "Nearest to over: lunches, yin, six, circ, malcolm, molinari, four, mosque,\n",
      "Nearest to also: often, it, which, lymphoma, agouti, circ, not, misery,\n",
      "Nearest to are: were, is, have, was, zero, although, tree, flaw,\n",
      "Nearest to new: agouti, election, birkenau, rushes, satisfied, aalen, albury, apatosaurus,\n",
      "Nearest to three: five, four, six, eight, seven, two, zero, one,\n",
      "Nearest to b: d, UNK, circ, eight, foe, albury, except, liable,\n",
      "Nearest to first: numa, last, inlay, canaris, carry, transient, recast, biochemical,\n",
      "Nearest to it: he, there, this, which, zero, coke, they, not,\n",
      "Nearest to called: and, hobbyist, armistice, greenhouses, often, overthrows, wattle, cured,\n",
      "Average loss at step  42000 :  5.37618545449\n",
      "Average loss at step  44000 :  5.25237128401\n",
      "Average loss at step  46000 :  5.21688790834\n",
      "Average loss at step  48000 :  5.22588794756\n",
      "Average loss at step  50000 :  4.98484210038\n",
      "Nearest to with: and, for, in, or, on, by, from, among,\n",
      "Nearest to which: that, this, it, but, and, agouti, there, altenberg,\n",
      "Nearest to who: and, he, circ, which, trapezohedron, not, they, sometimes,\n",
      "Nearest to at: in, on, three, coin, four, average, and, during,\n",
      "Nearest to been: be, was, were, by, become, customs, primigenius, reply,\n",
      "Nearest to world: resulting, continuous, dining, rifles, dialectic, enki, demolished, klan,\n",
      "Nearest to five: four, six, three, eight, seven, zero, agouti, two,\n",
      "Nearest to over: lunches, yin, solicitation, four, six, malcolm, circ, three,\n",
      "Nearest to also: often, which, it, lymphoma, not, there, agouti, trademarks,\n",
      "Nearest to are: were, is, have, although, was, flaw, be, tree,\n",
      "Nearest to new: cheerleader, election, agouti, birkenau, satisfied, mlb, aalen, rushes,\n",
      "Nearest to three: four, five, two, six, seven, eight, one, operatorname,\n",
      "Nearest to b: d, circ, albury, foe, sx, liable, except, penn,\n",
      "Nearest to first: last, second, numa, recast, canaris, transient, gollancz, recitative,\n",
      "Nearest to it: he, this, there, which, coke, agouti, they, six,\n",
      "Nearest to called: and, hobbyist, armistice, often, dasyprocta, greenhouses, overthrows, described,\n",
      "Average loss at step  52000 :  5.04872216845\n",
      "Average loss at step  54000 :  5.1925075227\n",
      "Average loss at step  56000 :  5.05047903585\n",
      "Average loss at step  58000 :  5.03660546994\n",
      "Average loss at step  60000 :  4.95552955997\n",
      "Nearest to with: in, or, for, michelob, ursus, and, among, dasyprocta,\n",
      "Nearest to which: this, that, it, but, also, one, agouti, there,\n",
      "Nearest to who: he, which, circ, they, and, not, kornilov, trapezohedron,\n",
      "Nearest to at: in, three, on, during, under, ursus, coin, average,\n",
      "Nearest to been: be, was, were, by, become, customs, primigenius, had,\n",
      "Nearest to world: continuous, resulting, dining, rifles, straightforward, dialectic, enki, klan,\n",
      "Nearest to five: six, four, three, eight, seven, zero, nine, two,\n",
      "Nearest to over: lunches, six, four, yin, eight, solicitation, circ, malcolm,\n",
      "Nearest to also: often, which, it, agouti, circ, lymphoma, usually, trademarks,\n",
      "Nearest to are: were, is, have, although, flaw, advanced, be, agave,\n",
      "Nearest to new: wct, cheerleader, agouti, election, birkenau, mlb, apatosaurus, rushes,\n",
      "Nearest to three: five, four, two, six, seven, eight, one, operatorname,\n",
      "Nearest to b: d, circ, incompletely, albury, UNK, sx, penn, foe,\n",
      "Nearest to first: last, second, recast, numa, gollancz, canaris, recitative, erebus,\n",
      "Nearest to it: he, this, there, which, they, coke, agouti, she,\n",
      "Nearest to called: hobbyist, armistice, overthrows, often, and, described, dasyprocta, cured,\n",
      "Average loss at step  62000 :  5.01795846856\n",
      "Average loss at step  64000 :  4.84104190159\n",
      "Average loss at step  66000 :  4.60777492678\n",
      "Average loss at step  68000 :  4.98473761606\n",
      "Average loss at step  70000 :  4.89423776472\n",
      "Nearest to with: in, and, michelob, between, ursus, for, or, saa,\n",
      "Nearest to which: that, this, it, but, also, there, one, usually,\n",
      "Nearest to who: he, circ, which, they, kornilov, not, sometimes, trapezohedron,\n",
      "Nearest to at: in, three, during, under, leontopithecus, on, ursus, after,\n",
      "Nearest to been: be, was, were, become, by, customs, primigenius, had,\n",
      "Nearest to world: resulting, continuous, dining, straightforward, dialectic, ranches, rifles, influences,\n",
      "Nearest to five: four, six, three, eight, seven, zero, two, nine,\n",
      "Nearest to over: lunches, yin, six, four, circ, solicitation, mosque, eight,\n",
      "Nearest to also: often, which, usually, circ, agouti, now, it, lymphoma,\n",
      "Nearest to are: were, is, have, be, although, was, advanced, cebus,\n",
      "Nearest to new: wct, agouti, cheerleader, cebus, sustain, rushes, aalen, apatosaurus,\n",
      "Nearest to three: five, four, six, two, eight, seven, nine, zero,\n",
      "Nearest to b: d, UNK, circ, cebus, incompletely, seven, albury, sx,\n",
      "Nearest to first: last, second, microcebus, recast, numa, jubilees, gollancz, adom,\n",
      "Nearest to it: he, there, this, which, they, coke, she, we,\n",
      "Nearest to called: hobbyist, overthrows, armistice, dasyprocta, described, and, assist, creative,\n",
      "Average loss at step  72000 :  4.75391916478\n",
      "Average loss at step  74000 :  4.80604923213\n",
      "Average loss at step  76000 :  4.73732077646\n",
      "Average loss at step  78000 :  4.79006587577\n",
      "Average loss at step  80000 :  4.78206545174\n",
      "Nearest to with: in, michelob, between, ursus, for, or, and, by,\n",
      "Nearest to which: that, this, it, but, also, usually, there, and,\n",
      "Nearest to who: he, they, circ, which, kornilov, de, carabiners, trapezohedron,\n",
      "Nearest to at: in, during, under, on, after, leontopithecus, fdi, ursus,\n",
      "Nearest to been: be, were, was, become, by, customs, primigenius, reply,\n",
      "Nearest to world: resulting, dining, continuous, ranches, straightforward, dialectic, influences, rifles,\n",
      "Nearest to five: four, six, three, seven, eight, nine, zero, two,\n",
      "Nearest to over: lunches, four, six, yin, circ, solicitation, mosque, molinari,\n",
      "Nearest to also: often, which, usually, now, it, agouti, circ, never,\n",
      "Nearest to are: were, is, have, although, be, was, flaw, solidly,\n",
      "Nearest to new: wct, agouti, cebus, aalen, apatosaurus, microcebus, birkenau, sustain,\n",
      "Nearest to three: five, four, six, two, seven, eight, one, nine,\n",
      "Nearest to b: d, circ, incompletely, cebus, UNK, albury, sx, penn,\n",
      "Nearest to first: last, second, recast, microcebus, jubilees, later, gollancz, recitative,\n",
      "Nearest to it: he, there, this, they, which, she, coke, we,\n",
      "Nearest to called: hobbyist, dasyprocta, often, and, described, overthrows, claus, creative,\n",
      "Average loss at step  82000 :  4.77122810602\n",
      "Average loss at step  84000 :  4.74881280625\n",
      "Average loss at step  86000 :  4.76137400806\n",
      "Average loss at step  88000 :  4.73941707897\n",
      "Average loss at step  90000 :  4.79837369692\n",
      "Nearest to with: michelob, in, between, ursus, by, among, microbats, microcebus,\n",
      "Nearest to which: that, this, but, it, also, and, there, where,\n",
      "Nearest to who: he, they, which, circ, and, kornilov, she, there,\n",
      "Nearest to at: in, during, under, on, after, three, leontopithecus, ursus,\n",
      "Nearest to been: be, was, become, were, by, primigenius, customs, reply,\n",
      "Nearest to world: resulting, continuous, dining, straightforward, influences, klan, venue, dignity,\n",
      "Nearest to five: four, seven, eight, six, three, nine, zero, two,\n",
      "Nearest to over: lunches, six, four, yin, solicitation, circ, molinari, autistic,\n",
      "Nearest to also: often, which, usually, now, therefore, it, agouti, never,\n",
      "Nearest to are: were, have, is, although, be, solidly, include, advanced,\n",
      "Nearest to new: wct, agouti, aalen, cebus, microcebus, apatosaurus, pontificia, cheerleader,\n",
      "Nearest to three: four, five, two, six, seven, eight, one, nine,\n",
      "Nearest to b: d, incompletely, circ, cebus, penn, albury, UNK, four,\n",
      "Nearest to first: second, last, recast, later, microcebus, gollancz, jubilees, next,\n",
      "Nearest to it: he, this, there, she, they, which, recitative, but,\n",
      "Nearest to called: dasyprocta, used, hobbyist, described, claus, capitan, and, agouti,\n",
      "Average loss at step  92000 :  4.65793276811\n",
      "Average loss at step  94000 :  4.71828448689\n",
      "Average loss at step  96000 :  4.68348034775\n",
      "Average loss at step  98000 :  4.60051974714\n",
      "Average loss at step  100000 :  4.69332761848\n",
      "Nearest to with: in, between, michelob, by, ursus, for, among, microcebus,\n",
      "Nearest to which: that, this, but, it, also, iit, where, usually,\n",
      "Nearest to who: he, they, circ, kornilov, wronged, which, she, carabiners,\n",
      "Nearest to at: in, during, under, after, on, leontopithecus, ursus, from,\n",
      "Nearest to been: be, become, was, were, by, primigenius, reply, customs,\n",
      "Nearest to world: dining, continuous, resulting, influences, straightforward, dignity, venue, dialectic,\n",
      "Nearest to five: four, seven, three, six, eight, two, zero, nine,\n",
      "Nearest to over: four, lunches, yin, circ, consolidate, solicitation, molinari, autistic,\n",
      "Nearest to also: often, which, usually, now, therefore, circ, it, sometimes,\n",
      "Nearest to are: were, is, have, although, solidly, include, be, peacocks,\n",
      "Nearest to new: wct, agouti, aalen, cheerleader, apatosaurus, microcebus, cebus, rushes,\n",
      "Nearest to three: five, four, seven, two, six, eight, nine, operatorname,\n",
      "Nearest to b: d, circ, incompletely, cebus, albury, penn, sx, four,\n",
      "Nearest to first: second, last, recast, microcebus, later, jubilees, gollancz, next,\n",
      "Nearest to it: he, this, there, she, they, which, recitative, agouti,\n",
      "Nearest to called: and, overthrows, used, dasyprocta, described, capitan, hobbyist, claus,\n",
      "Please install sklearn, matplotlib, and scipy to show embeddings.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기네요...\n",
    "\n",
    "Step 별로 해석을 해보겠습니다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download the data.\n",
    "\n",
    "다운로드는 패스하고 자료 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9094751e0b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 소스에 메모리 이슈로 `del vocabulary` 을 해서 not defined가 뜸. 이런 팁은 배워둘만 한 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anarchism'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아, 엄청 큰 글을 그냥 단어로 나눠버린거구나.. 사실 원한 그림은 paragraph2vec과 같은 거였는데 지금은 패스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "\n",
    "UNK는 unknown의 약자란다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = vocabulary\n",
    "n_words = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = [['UNK', -1]]\n",
    "count.extend(collections.Counter(words).most_common(n_words - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', -1],\n",
       " ('the', 1061396),\n",
       " ('of', 593677),\n",
       " ('and', 416629),\n",
       " ('one', 411764)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 분포수겠지. 굳이 정말인가 확인해보려면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for w in words:\n",
    "    if w == 'the':\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061396"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네 맞습니다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "print (dictionary['UNK'], dictionary['the'], dictionary['of'], dictionary['and'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어에 차례대로 숫자를 부여하는 부분이고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list()\n",
    "unk_count = 0\n",
    "for word in words:\n",
    "    if word in dictionary:\n",
    "        index = dictionary[word]\n",
    "    else:\n",
    "        index = 0  # dictionary['UNK']\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0][1] = unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism', 'originated', 'as']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5244, 3083, 12]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5244 3083 12\n"
     ]
    }
   ],
   "source": [
    "print (dictionary['anarchism'], dictionary['originated'], dictionary['as'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저 부분은 words (vocabulary)의 각 단어에다가 위에서 부여한 단어의 번호(인덱스)를 부여하는 부분이다.\n",
    "\n",
    "\n",
    "이게 어디서 쓰이는지는 밑에서 봐야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: 'the',\n",
       " 2: 'of',\n",
       " 3: 'and',\n",
       " 4: 'one',\n",
       " 5: 'in',\n",
       " 6: 'a',\n",
       " 7: 'to',\n",
       " 8: 'zero',\n",
       " 9: 'nine',\n",
       " 10: 'two',\n",
       " 11: 'is',\n",
       " 12: 'as',\n",
       " 13: 'eight',\n",
       " 14: 'for',\n",
       " 15: 's',\n",
       " 16: 'five',\n",
       " 17: 'three',\n",
       " 18: 'was',\n",
       " 19: 'by',\n",
       " 20: 'that',\n",
       " 21: 'four',\n",
       " 22: 'six',\n",
       " 23: 'seven',\n",
       " 24: 'with',\n",
       " 25: 'on',\n",
       " 26: 'are',\n",
       " 27: 'it',\n",
       " 28: 'from',\n",
       " 29: 'or',\n",
       " 30: 'his',\n",
       " 31: 'an',\n",
       " 32: 'be',\n",
       " 33: 'this',\n",
       " 34: 'which',\n",
       " 35: 'at',\n",
       " 36: 'he',\n",
       " 37: 'also',\n",
       " 38: 'not',\n",
       " 39: 'have',\n",
       " 40: 'were',\n",
       " 41: 'has',\n",
       " 42: 'but',\n",
       " 43: 'other',\n",
       " 44: 'their',\n",
       " 45: 'its',\n",
       " 46: 'first',\n",
       " 47: 'they',\n",
       " 48: 'some',\n",
       " 49: 'had',\n",
       " 50: 'all',\n",
       " 51: 'more',\n",
       " 52: 'most',\n",
       " 53: 'can',\n",
       " 54: 'been',\n",
       " 55: 'such',\n",
       " 56: 'many',\n",
       " 57: 'who',\n",
       " 58: 'new',\n",
       " 59: 'used',\n",
       " 60: 'there',\n",
       " 61: 'after',\n",
       " 62: 'when',\n",
       " 63: 'into',\n",
       " 64: 'american',\n",
       " 65: 'time',\n",
       " 66: 'these',\n",
       " 67: 'only',\n",
       " 68: 'see',\n",
       " 69: 'may',\n",
       " 70: 'than',\n",
       " 71: 'world',\n",
       " 72: 'i',\n",
       " 73: 'b',\n",
       " 74: 'would',\n",
       " 75: 'd',\n",
       " 76: 'no',\n",
       " 77: 'however',\n",
       " 78: 'between',\n",
       " 79: 'about',\n",
       " 80: 'over',\n",
       " 81: 'years',\n",
       " 82: 'states',\n",
       " 83: 'people',\n",
       " 84: 'war',\n",
       " 85: 'during',\n",
       " 86: 'united',\n",
       " 87: 'known',\n",
       " 88: 'if',\n",
       " 89: 'called',\n",
       " 90: 'use',\n",
       " 91: 'th',\n",
       " 92: 'system',\n",
       " 93: 'often',\n",
       " 94: 'state',\n",
       " 95: 'so',\n",
       " 96: 'history',\n",
       " 97: 'will',\n",
       " 98: 'up',\n",
       " 99: 'while',\n",
       " 100: 'where',\n",
       " 101: 'city',\n",
       " 102: 'being',\n",
       " 103: 'english',\n",
       " 104: 'then',\n",
       " 105: 'any',\n",
       " 106: 'both',\n",
       " 107: 'under',\n",
       " 108: 'out',\n",
       " 109: 'made',\n",
       " 110: 'well',\n",
       " 111: 'her',\n",
       " 112: 'e',\n",
       " 113: 'number',\n",
       " 114: 'government',\n",
       " 115: 'them',\n",
       " 116: 'm',\n",
       " 117: 'later',\n",
       " 118: 'since',\n",
       " 119: 'him',\n",
       " 120: 'part',\n",
       " 121: 'name',\n",
       " 122: 'c',\n",
       " 123: 'century',\n",
       " 124: 'through',\n",
       " 125: 'because',\n",
       " 126: 'x',\n",
       " 127: 'university',\n",
       " 128: 'early',\n",
       " 129: 'life',\n",
       " 130: 'british',\n",
       " 131: 'year',\n",
       " 132: 'like',\n",
       " 133: 'same',\n",
       " 134: 'including',\n",
       " 135: 'became',\n",
       " 136: 'example',\n",
       " 137: 'day',\n",
       " 138: 'each',\n",
       " 139: 'even',\n",
       " 140: 'work',\n",
       " 141: 'language',\n",
       " 142: 'although',\n",
       " 143: 'several',\n",
       " 144: 'form',\n",
       " 145: 'john',\n",
       " 146: 'u',\n",
       " 147: 'national',\n",
       " 148: 'very',\n",
       " 149: 'much',\n",
       " 150: 'g',\n",
       " 151: 'french',\n",
       " 152: 'before',\n",
       " 153: 'general',\n",
       " 154: 'what',\n",
       " 155: 't',\n",
       " 156: 'against',\n",
       " 157: 'n',\n",
       " 158: 'high',\n",
       " 159: 'links',\n",
       " 160: 'could',\n",
       " 161: 'based',\n",
       " 162: 'those',\n",
       " 163: 'now',\n",
       " 164: 'second',\n",
       " 165: 'de',\n",
       " 166: 'music',\n",
       " 167: 'another',\n",
       " 168: 'large',\n",
       " 169: 'she',\n",
       " 170: 'f',\n",
       " 171: 'external',\n",
       " 172: 'german',\n",
       " 173: 'different',\n",
       " 174: 'modern',\n",
       " 175: 'great',\n",
       " 176: 'do',\n",
       " 177: 'common',\n",
       " 178: 'set',\n",
       " 179: 'list',\n",
       " 180: 'south',\n",
       " 181: 'series',\n",
       " 182: 'major',\n",
       " 183: 'game',\n",
       " 184: 'power',\n",
       " 185: 'long',\n",
       " 186: 'country',\n",
       " 187: 'king',\n",
       " 188: 'law',\n",
       " 189: 'group',\n",
       " 190: 'film',\n",
       " 191: 'still',\n",
       " 192: 'until',\n",
       " 193: 'north',\n",
       " 194: 'international',\n",
       " 195: 'term',\n",
       " 196: 'we',\n",
       " 197: 'end',\n",
       " 198: 'book',\n",
       " 199: 'found',\n",
       " 200: 'own',\n",
       " 201: 'political',\n",
       " 202: 'party',\n",
       " 203: 'order',\n",
       " 204: 'usually',\n",
       " 205: 'president',\n",
       " 206: 'church',\n",
       " 207: 'you',\n",
       " 208: 'death',\n",
       " 209: 'theory',\n",
       " 210: 'area',\n",
       " 211: 'around',\n",
       " 212: 'include',\n",
       " 213: 'god',\n",
       " 214: 'ii',\n",
       " 215: 'way',\n",
       " 216: 'did',\n",
       " 217: 'military',\n",
       " 218: 'population',\n",
       " 219: 'using',\n",
       " 220: 'though',\n",
       " 221: 'small',\n",
       " 222: 'following',\n",
       " 223: 'within',\n",
       " 224: 'non',\n",
       " 225: 'human',\n",
       " 226: 'left',\n",
       " 227: 'main',\n",
       " 228: 'among',\n",
       " 229: 'point',\n",
       " 230: 'r',\n",
       " 231: 'due',\n",
       " 232: 'p',\n",
       " 233: 'considered',\n",
       " 234: 'public',\n",
       " 235: 'popular',\n",
       " 236: 'computer',\n",
       " 237: 'west',\n",
       " 238: 'family',\n",
       " 239: 'east',\n",
       " 240: 'information',\n",
       " 241: 'important',\n",
       " 242: 'european',\n",
       " 243: 'man',\n",
       " 244: 'sometimes',\n",
       " 245: 'right',\n",
       " 246: 'old',\n",
       " 247: 'free',\n",
       " 248: 'word',\n",
       " 249: 'without',\n",
       " 250: 'last',\n",
       " 251: 'us',\n",
       " 252: 'members',\n",
       " 253: 'given',\n",
       " 254: 'times',\n",
       " 255: 'roman',\n",
       " 256: 'make',\n",
       " 257: 'h',\n",
       " 258: 'age',\n",
       " 259: 'place',\n",
       " 260: 'l',\n",
       " 261: 'thus',\n",
       " 262: 'science',\n",
       " 263: 'case',\n",
       " 264: 'become',\n",
       " 265: 'systems',\n",
       " 266: 'union',\n",
       " 267: 'born',\n",
       " 268: 'york',\n",
       " 269: 'line',\n",
       " 270: 'countries',\n",
       " 271: 'does',\n",
       " 272: 'isbn',\n",
       " 273: 'st',\n",
       " 274: 'control',\n",
       " 275: 'various',\n",
       " 276: 'others',\n",
       " 277: 'house',\n",
       " 278: 'article',\n",
       " 279: 'island',\n",
       " 280: 'should',\n",
       " 281: 'led',\n",
       " 282: 'back',\n",
       " 283: 'period',\n",
       " 284: 'player',\n",
       " 285: 'europe',\n",
       " 286: 'languages',\n",
       " 287: 'central',\n",
       " 288: 'water',\n",
       " 289: 'few',\n",
       " 290: 'western',\n",
       " 291: 'home',\n",
       " 292: 'began',\n",
       " 293: 'generally',\n",
       " 294: 'less',\n",
       " 295: 'k',\n",
       " 296: 'similar',\n",
       " 297: 'written',\n",
       " 298: 'original',\n",
       " 299: 'best',\n",
       " 300: 'must',\n",
       " 301: 'according',\n",
       " 302: 'school',\n",
       " 303: 'france',\n",
       " 304: 'air',\n",
       " 305: 'single',\n",
       " 306: 'force',\n",
       " 307: 'v',\n",
       " 308: 'land',\n",
       " 309: 'groups',\n",
       " 310: 'down',\n",
       " 311: 'how',\n",
       " 312: 'works',\n",
       " 313: 'development',\n",
       " 314: 'official',\n",
       " 315: 'support',\n",
       " 316: 'england',\n",
       " 317: 'j',\n",
       " 318: 'rather',\n",
       " 319: 'space',\n",
       " 320: 'data',\n",
       " 321: 'greek',\n",
       " 322: 'km',\n",
       " 323: 'named',\n",
       " 324: 'germany',\n",
       " 325: 'just',\n",
       " 326: 'games',\n",
       " 327: 'said',\n",
       " 328: 'version',\n",
       " 329: 'late',\n",
       " 330: 'earth',\n",
       " 331: 'company',\n",
       " 332: 'every',\n",
       " 333: 'economic',\n",
       " 334: 'short',\n",
       " 335: 'published',\n",
       " 336: 'black',\n",
       " 337: 'army',\n",
       " 338: 'off',\n",
       " 339: 'london',\n",
       " 340: 'million',\n",
       " 341: 'body',\n",
       " 342: 'field',\n",
       " 343: 'christian',\n",
       " 344: 'either',\n",
       " 345: 'social',\n",
       " 346: 'empire',\n",
       " 347: 'o',\n",
       " 348: 'developed',\n",
       " 349: 'standard',\n",
       " 350: 'court',\n",
       " 351: 'service',\n",
       " 352: 'kingdom',\n",
       " 353: 'along',\n",
       " 354: 'college',\n",
       " 355: 'republic',\n",
       " 356: 'sea',\n",
       " 357: 'america',\n",
       " 358: 'today',\n",
       " 359: 'result',\n",
       " 360: 'held',\n",
       " 361: 'team',\n",
       " 362: 'light',\n",
       " 363: 'means',\n",
       " 364: 'never',\n",
       " 365: 'especially',\n",
       " 366: 'third',\n",
       " 367: 'further',\n",
       " 368: 'character',\n",
       " 369: 'forces',\n",
       " 370: 'take',\n",
       " 371: 'men',\n",
       " 372: 'society',\n",
       " 373: 'show',\n",
       " 374: 'open',\n",
       " 375: 'possible',\n",
       " 376: 'fact',\n",
       " 377: 'battle',\n",
       " 378: 'took',\n",
       " 379: 'former',\n",
       " 380: 'books',\n",
       " 381: 'soviet',\n",
       " 382: 'river',\n",
       " 383: 'children',\n",
       " 384: 'having',\n",
       " 385: 'good',\n",
       " 386: 'local',\n",
       " 387: 'current',\n",
       " 388: 'son',\n",
       " 389: 'process',\n",
       " 390: 'natural',\n",
       " 391: 'present',\n",
       " 392: 'himself',\n",
       " 393: 'islands',\n",
       " 394: 'total',\n",
       " 395: 'near',\n",
       " 396: 'white',\n",
       " 397: 'days',\n",
       " 398: 'person',\n",
       " 399: 'itself',\n",
       " 400: 'seen',\n",
       " 401: 'culture',\n",
       " 402: 'little',\n",
       " 403: 'above',\n",
       " 404: 'software',\n",
       " 405: 'largest',\n",
       " 406: 'words',\n",
       " 407: 'upon',\n",
       " 408: 'level',\n",
       " 409: 'father',\n",
       " 410: 'side',\n",
       " 411: 'created',\n",
       " 412: 'red',\n",
       " 413: 'references',\n",
       " 414: 'press',\n",
       " 415: 'full',\n",
       " 416: 'region',\n",
       " 417: 'almost',\n",
       " 418: 'al',\n",
       " 419: 'image',\n",
       " 420: 'famous',\n",
       " 421: 'play',\n",
       " 422: 'came',\n",
       " 423: 'role',\n",
       " 424: 'once',\n",
       " 425: 'certain',\n",
       " 426: 'league',\n",
       " 427: 'jewish',\n",
       " 428: 'james',\n",
       " 429: 'january',\n",
       " 430: 'site',\n",
       " 431: 'again',\n",
       " 432: 'art',\n",
       " 433: 'numbers',\n",
       " 434: 'member',\n",
       " 435: 'areas',\n",
       " 436: 'movement',\n",
       " 437: 'religious',\n",
       " 438: 'type',\n",
       " 439: 'march',\n",
       " 440: 'community',\n",
       " 441: 'story',\n",
       " 442: 'played',\n",
       " 443: 'production',\n",
       " 444: 'released',\n",
       " 445: 'center',\n",
       " 446: 'rights',\n",
       " 447: 'real',\n",
       " 448: 'related',\n",
       " 449: 'foreign',\n",
       " 450: 'low',\n",
       " 451: 'ancient',\n",
       " 452: 'view',\n",
       " 453: 'terms',\n",
       " 454: 'source',\n",
       " 455: 'act',\n",
       " 456: 'minister',\n",
       " 457: 'change',\n",
       " 458: 'energy',\n",
       " 459: 'produced',\n",
       " 460: 'research',\n",
       " 461: 'actor',\n",
       " 462: 'making',\n",
       " 463: 'civil',\n",
       " 464: 'december',\n",
       " 465: 'women',\n",
       " 466: 'special',\n",
       " 467: 'style',\n",
       " 468: 'william',\n",
       " 469: 'design',\n",
       " 470: 'japanese',\n",
       " 471: 'available',\n",
       " 472: 'chinese',\n",
       " 473: 'forms',\n",
       " 474: 'canada',\n",
       " 475: 'northern',\n",
       " 476: 'died',\n",
       " 477: 'class',\n",
       " 478: 'living',\n",
       " 479: 'next',\n",
       " 480: 'particular',\n",
       " 481: 'program',\n",
       " 482: 'council',\n",
       " 483: 'television',\n",
       " 484: 'head',\n",
       " 485: 'david',\n",
       " 486: 'china',\n",
       " 487: 'middle',\n",
       " 488: 'established',\n",
       " 489: 'hand',\n",
       " 490: 'bc',\n",
       " 491: 'far',\n",
       " 492: 'july',\n",
       " 493: 'function',\n",
       " 494: 'position',\n",
       " 495: 'y',\n",
       " 496: 'built',\n",
       " 497: 'george',\n",
       " 498: 'band',\n",
       " 499: 'together',\n",
       " 500: 'w',\n",
       " 501: 'latin',\n",
       " 502: 'thought',\n",
       " 503: 'eastern',\n",
       " 504: 'charles',\n",
       " 505: 'parts',\n",
       " 506: 'instead',\n",
       " 507: 'study',\n",
       " 508: 'might',\n",
       " 509: 'india',\n",
       " 510: 'code',\n",
       " 511: 'meaning',\n",
       " 512: 'included',\n",
       " 513: 'trade',\n",
       " 514: 'per',\n",
       " 515: 'june',\n",
       " 516: 'least',\n",
       " 517: 'half',\n",
       " 518: 'model',\n",
       " 519: 'economy',\n",
       " 520: 'prime',\n",
       " 521: 'traditional',\n",
       " 522: 'always',\n",
       " 523: 'capital',\n",
       " 524: 'range',\n",
       " 525: 'emperor',\n",
       " 526: 'november',\n",
       " 527: 'young',\n",
       " 528: 'anti',\n",
       " 529: 'final',\n",
       " 530: 'text',\n",
       " 531: 'players',\n",
       " 532: 'uk',\n",
       " 533: 'april',\n",
       " 534: 'run',\n",
       " 535: 'september',\n",
       " 536: 'radio',\n",
       " 537: 'addition',\n",
       " 538: 'live',\n",
       " 539: 'august',\n",
       " 540: 'note',\n",
       " 541: 'taken',\n",
       " 542: 'italian',\n",
       " 543: 'lost',\n",
       " 544: 'nature',\n",
       " 545: 'project',\n",
       " 546: 'technology',\n",
       " 547: 'spanish',\n",
       " 548: 'october',\n",
       " 549: 'rate',\n",
       " 550: 'recent',\n",
       " 551: 'won',\n",
       " 552: 'true',\n",
       " 553: 'value',\n",
       " 554: 'uses',\n",
       " 555: 'russian',\n",
       " 556: 'est',\n",
       " 557: 'wrote',\n",
       " 558: 'effect',\n",
       " 559: 'album',\n",
       " 560: 'southern',\n",
       " 561: 'africa',\n",
       " 562: 'whose',\n",
       " 563: 'top',\n",
       " 564: 'historical',\n",
       " 565: 'australia',\n",
       " 566: 'catholic',\n",
       " 567: 'particularly',\n",
       " 568: 'self',\n",
       " 569: 'structure',\n",
       " 570: 'record',\n",
       " 571: 'evidence',\n",
       " 572: 'rule',\n",
       " 573: 'themselves',\n",
       " 574: 'influence',\n",
       " 575: 'cases',\n",
       " 576: 'subject',\n",
       " 577: 'referred',\n",
       " 578: 'continued',\n",
       " 579: 'nations',\n",
       " 580: 'rock',\n",
       " 581: 'below',\n",
       " 582: 'japan',\n",
       " 583: 'com',\n",
       " 584: 'song',\n",
       " 585: 'throughout',\n",
       " 586: 'names',\n",
       " 587: 'female',\n",
       " 588: 'title',\n",
       " 589: 'therefore',\n",
       " 590: 'our',\n",
       " 591: 'office',\n",
       " 592: 'star',\n",
       " 593: 'paul',\n",
       " 594: 'too',\n",
       " 595: 'cities',\n",
       " 596: 'february',\n",
       " 597: 'independent',\n",
       " 598: 'author',\n",
       " 599: 'problem',\n",
       " 600: 'species',\n",
       " 601: 'education',\n",
       " 602: 'done',\n",
       " 603: 'philosophy',\n",
       " 604: 'come',\n",
       " 605: 'higher',\n",
       " 606: 'originally',\n",
       " 607: 'market',\n",
       " 608: 'town',\n",
       " 609: 'my',\n",
       " 610: 'season',\n",
       " 611: 'love',\n",
       " 612: 'strong',\n",
       " 613: 'israel',\n",
       " 614: 'writer',\n",
       " 615: 'irish',\n",
       " 616: 'films',\n",
       " 617: 'elements',\n",
       " 618: 'robert',\n",
       " 619: 'whether',\n",
       " 620: 'despite',\n",
       " 621: 'eventually',\n",
       " 622: 'here',\n",
       " 623: 'football',\n",
       " 624: 'action',\n",
       " 625: 'internet',\n",
       " 626: 'individual',\n",
       " 627: 'sound',\n",
       " 628: 'network',\n",
       " 629: 'described',\n",
       " 630: 'practice',\n",
       " 631: 'characters',\n",
       " 632: 're',\n",
       " 633: 'royal',\n",
       " 634: 'la',\n",
       " 635: 'events',\n",
       " 636: 'formed',\n",
       " 637: 'commonly',\n",
       " 638: 'base',\n",
       " 639: 'received',\n",
       " 640: 'african',\n",
       " 641: 'problems',\n",
       " 642: 'food',\n",
       " 643: 'jews',\n",
       " 644: 'able',\n",
       " 645: 'male',\n",
       " 646: 'typically',\n",
       " 647: 'mass',\n",
       " 648: 'complex',\n",
       " 649: 'lower',\n",
       " 650: 'includes',\n",
       " 651: 'outside',\n",
       " 652: 'legal',\n",
       " 653: 'complete',\n",
       " 654: 'parliament',\n",
       " 655: 'significant',\n",
       " 656: 'actually',\n",
       " 657: 'business',\n",
       " 658: 'fiction',\n",
       " 659: 'physical',\n",
       " 660: 'followed',\n",
       " 661: 'deaths',\n",
       " 662: 'key',\n",
       " 663: 'widely',\n",
       " 664: 'leader',\n",
       " 665: 'page',\n",
       " 666: 'basic',\n",
       " 667: 'types',\n",
       " 668: 'henry',\n",
       " 669: 'beginning',\n",
       " 670: 'elected',\n",
       " 671: 'fire',\n",
       " 672: 'building',\n",
       " 673: 'independence',\n",
       " 674: 'went',\n",
       " 675: 'aircraft',\n",
       " 676: 'movie',\n",
       " 677: 'ever',\n",
       " 678: 'canadian',\n",
       " 679: 'material',\n",
       " 680: 'births',\n",
       " 681: 'video',\n",
       " 682: 'news',\n",
       " 683: 'future',\n",
       " 684: 'scientific',\n",
       " 685: 'simply',\n",
       " 686: 'go',\n",
       " 687: 'defined',\n",
       " 688: 'laws',\n",
       " 689: 'get',\n",
       " 690: 'close',\n",
       " 691: 'industry',\n",
       " 692: 'specific',\n",
       " 693: 'examples',\n",
       " 694: 'believe',\n",
       " 695: 'services',\n",
       " 696: 'idea',\n",
       " 697: 'method',\n",
       " 698: 'introduced',\n",
       " 699: 'points',\n",
       " 700: 'return',\n",
       " 701: 'cause',\n",
       " 702: 'indian',\n",
       " 703: 'britain',\n",
       " 704: 'features',\n",
       " 705: 'size',\n",
       " 706: 'majority',\n",
       " 707: 'post',\n",
       " 708: 'lead',\n",
       " 709: 'organization',\n",
       " 710: 'cannot',\n",
       " 711: 'designed',\n",
       " 712: 'ireland',\n",
       " 713: 'cross',\n",
       " 714: 'classical',\n",
       " 715: 'personal',\n",
       " 716: 'writing',\n",
       " 717: 'concept',\n",
       " 718: 'associated',\n",
       " 719: 'required',\n",
       " 720: 'soon',\n",
       " 721: 'changes',\n",
       " 722: 'california',\n",
       " 723: 'located',\n",
       " 724: 'sense',\n",
       " 725: 'believed',\n",
       " 726: 'away',\n",
       " 727: 'started',\n",
       " 728: 'co',\n",
       " 729: 'religion',\n",
       " 730: 'mother',\n",
       " 731: 'county',\n",
       " 732: 'rules',\n",
       " 733: 'studies',\n",
       " 734: 'yet',\n",
       " 735: 'find',\n",
       " 736: 'knowledge',\n",
       " 737: 'put',\n",
       " 738: 'founded',\n",
       " 739: 'policy',\n",
       " 740: 'currently',\n",
       " 741: 'provide',\n",
       " 742: 'working',\n",
       " 743: 'media',\n",
       " 744: 'election',\n",
       " 745: 'australian',\n",
       " 746: 'me',\n",
       " 747: 'thomas',\n",
       " 748: 'allowed',\n",
       " 749: 'russia',\n",
       " 750: 'earlier',\n",
       " 751: 'greater',\n",
       " 752: 'limited',\n",
       " 753: 'object',\n",
       " 754: 'brought',\n",
       " 755: 'online',\n",
       " 756: 'association',\n",
       " 757: 'lord',\n",
       " 758: 'mostly',\n",
       " 759: 'blue',\n",
       " 760: 'constitution',\n",
       " 761: 'across',\n",
       " 762: 'added',\n",
       " 763: 'interest',\n",
       " 764: 'things',\n",
       " 765: 'relations',\n",
       " 766: 'speed',\n",
       " 767: 'federal',\n",
       " 768: 'singer',\n",
       " 769: 'effects',\n",
       " 770: 'growth',\n",
       " 771: 'sources',\n",
       " 772: 'your',\n",
       " 773: 'remains',\n",
       " 774: 'z',\n",
       " 775: 'gave',\n",
       " 776: 'probably',\n",
       " 777: 'simple',\n",
       " 778: 'attack',\n",
       " 779: 'longer',\n",
       " 780: 'reference',\n",
       " 781: 'saint',\n",
       " 782: 'success',\n",
       " 783: 'killed',\n",
       " 784: 'career',\n",
       " 785: 'past',\n",
       " 786: 'need',\n",
       " 787: 'park',\n",
       " 788: 'definition',\n",
       " 789: 'say',\n",
       " 790: 'etc',\n",
       " 791: 'peace',\n",
       " 792: 'give',\n",
       " 793: 'chief',\n",
       " 794: 'stories',\n",
       " 795: 'security',\n",
       " 796: 'wide',\n",
       " 797: 'ball',\n",
       " 798: 'saw',\n",
       " 799: 'machine',\n",
       " 800: 'better',\n",
       " 801: 'cell',\n",
       " 802: 'leading',\n",
       " 803: 'becomes',\n",
       " 804: 'larger',\n",
       " 805: 'spain',\n",
       " 806: 'night',\n",
       " 807: 'products',\n",
       " 808: 'parties',\n",
       " 809: 'remained',\n",
       " 810: 'prize',\n",
       " 811: 'big',\n",
       " 812: 'website',\n",
       " 813: 'months',\n",
       " 814: 'money',\n",
       " 815: 'cultural',\n",
       " 816: 'territory',\n",
       " 817: 'help',\n",
       " 818: 'private',\n",
       " 819: 'moved',\n",
       " 820: 'wife',\n",
       " 821: 'letter',\n",
       " 822: 'lines',\n",
       " 823: 'politics',\n",
       " 824: 'largely',\n",
       " 825: 'contains',\n",
       " 826: 'companies',\n",
       " 827: 'lake',\n",
       " 828: 'perhaps',\n",
       " 829: 'green',\n",
       " 830: 'already',\n",
       " 831: 'iii',\n",
       " 832: 'dead',\n",
       " 833: 'library',\n",
       " 834: 'separate',\n",
       " 835: 'refer',\n",
       " 836: 'makes',\n",
       " 837: 'appeared',\n",
       " 838: 'dutch',\n",
       " 839: 'holy',\n",
       " 840: 'era',\n",
       " 841: 'novel',\n",
       " 842: 'successful',\n",
       " 843: 'italy',\n",
       " 844: 'letters',\n",
       " 845: 'results',\n",
       " 846: 'matter',\n",
       " 847: 'produce',\n",
       " 848: 'origin',\n",
       " 849: 'claim',\n",
       " 850: 'whole',\n",
       " 851: 'directly',\n",
       " 852: 'attempt',\n",
       " 853: 'actress',\n",
       " 854: 'surface',\n",
       " 855: 'revolution',\n",
       " 856: 'highly',\n",
       " 857: 'caused',\n",
       " 858: 'status',\n",
       " 859: 'musical',\n",
       " 860: 'richard',\n",
       " 861: 'commercial',\n",
       " 862: 'division',\n",
       " 863: 'color',\n",
       " 864: 'health',\n",
       " 865: 'coast',\n",
       " 866: 'release',\n",
       " 867: 'latter',\n",
       " 868: 'authority',\n",
       " 869: 'treaty',\n",
       " 870: 'turn',\n",
       " 871: 'michael',\n",
       " 872: 'nation',\n",
       " 873: 'direct',\n",
       " 874: 'asia',\n",
       " 875: 'edition',\n",
       " 876: 'programming',\n",
       " 877: 'playing',\n",
       " 878: 'date',\n",
       " 879: 'whom',\n",
       " 880: 'native',\n",
       " 881: 'mary',\n",
       " 882: 'married',\n",
       " 883: 'towards',\n",
       " 884: 'issues',\n",
       " 885: 'double',\n",
       " 886: 'basis',\n",
       " 887: 'primary',\n",
       " 888: 'allow',\n",
       " 889: 'enough',\n",
       " 890: 'memory',\n",
       " 891: 'reason',\n",
       " 892: 'web',\n",
       " 893: 'exist',\n",
       " 894: 'provided',\n",
       " 895: 'course',\n",
       " 896: 'functions',\n",
       " 897: 'oil',\n",
       " 898: 'chemical',\n",
       " 899: 'alexander',\n",
       " 900: 'analysis',\n",
       " 901: 'mid',\n",
       " 902: 'replaced',\n",
       " 903: 'queen',\n",
       " 904: 'sun',\n",
       " 905: 'claims',\n",
       " 906: 'tv',\n",
       " 907: 'literature',\n",
       " 908: 'metal',\n",
       " 909: 'amount',\n",
       " 910: 'divided',\n",
       " 911: 'blood',\n",
       " 912: 'likely',\n",
       " 913: 'access',\n",
       " 914: 'average',\n",
       " 915: 'length',\n",
       " 916: 'smaller',\n",
       " 917: 'medical',\n",
       " 918: 'property',\n",
       " 919: 'students',\n",
       " 920: 'degree',\n",
       " 921: 'elections',\n",
       " 922: 'club',\n",
       " 923: 'claimed',\n",
       " 924: 'performance',\n",
       " 925: 'director',\n",
       " 926: 'digital',\n",
       " 927: 'front',\n",
       " 928: 'museum',\n",
       " 929: 'tradition',\n",
       " 930: 'difficult',\n",
       " 931: 'nearly',\n",
       " 932: 'schools',\n",
       " 933: 'washington',\n",
       " 934: 'gas',\n",
       " 935: 'map',\n",
       " 936: 'jesus',\n",
       " 937: 'louis',\n",
       " 938: 'rome',\n",
       " 939: 'unit',\n",
       " 940: 'baseball',\n",
       " 941: 'mind',\n",
       " 942: 'peter',\n",
       " 943: 'mark',\n",
       " 944: 'collection',\n",
       " 945: 'product',\n",
       " 946: 'congress',\n",
       " 947: 'programs',\n",
       " 948: 'changed',\n",
       " 949: 'ideas',\n",
       " 950: 'moon',\n",
       " 951: 'entire',\n",
       " 952: 'user',\n",
       " 953: 'ground',\n",
       " 954: 'records',\n",
       " 955: 'frequently',\n",
       " 956: 'increase',\n",
       " 957: 'highest',\n",
       " 958: 'finally',\n",
       " 959: 'sent',\n",
       " 960: 'board',\n",
       " 961: 'don',\n",
       " 962: 'notable',\n",
       " 963: 'read',\n",
       " 964: 'methods',\n",
       " 965: 'recently',\n",
       " 966: 'bit',\n",
       " 967: 'variety',\n",
       " 968: 'involved',\n",
       " 969: 'call',\n",
       " 970: 'democratic',\n",
       " 971: 'ten',\n",
       " 972: 'served',\n",
       " 973: 'minor',\n",
       " 974: 'hard',\n",
       " 975: 'birth',\n",
       " 976: 'objects',\n",
       " 977: 'nuclear',\n",
       " 978: 'increased',\n",
       " 979: 'section',\n",
       " 980: 'street',\n",
       " 981: 'windows',\n",
       " 982: 'relatively',\n",
       " 983: 'car',\n",
       " 984: 'move',\n",
       " 985: 'create',\n",
       " 986: 'returned',\n",
       " 987: 'bank',\n",
       " 988: 'conditions',\n",
       " 989: 'operation',\n",
       " 990: 'adopted',\n",
       " 991: 'relationship',\n",
       " 992: 'christ',\n",
       " 993: 'hall',\n",
       " 994: 'appear',\n",
       " 995: 'rest',\n",
       " 996: 'child',\n",
       " 997: 'element',\n",
       " 998: 'appears',\n",
       " 999: 'takes',\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "num_skips=2\n",
    "skip_window=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global data_index\n",
    "assert batch_size % num_skips == 0\n",
    "assert num_skips <= 2 * skip_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assert는 예외처리를 나타내는 거라고 생각하면 된단다.  \n",
    "http://hashcode.co.kr/questions/958/assert%EB%8A%94-%EC%96%B8%EC%A0%9C-%EC%93%B0%EB%82%98%EC%9A%94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "buffer = collections.deque(maxlen=span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int32))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.ndarray는 numpy에서 쓰는 n차월 배열객체.  \n",
    "참조 http://yujuwon.tistory.com/entry/NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, deque([5244, 3083, 12]), 3)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span, buffer, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5244, 3083, 12, 6, 195, 2]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저 for문을 계속 돌리면 buffer에 3개씩 data의 값들이 순차적으로 들어가게 된다.\n",
    "\n",
    "실제로 저 for문이 있는 generate_batch 함수가 Step5에서 사용되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "targets_to_avoid : [1]\n",
      "0 0\n",
      "target : 0\n",
      "targets_to_avoid : [1, 0]\n",
      "0 1\n",
      "target : 2\n",
      "targets_to_avoid : [1, 0, 2]\n",
      "data_index : 3\n",
      "data[data_index] : 6\n",
      "1\n",
      "targets_to_avoid : [1]\n",
      "1 0\n",
      "target : 2\n",
      "targets_to_avoid : [1, 2]\n",
      "1 1\n",
      "target : 0\n",
      "targets_to_avoid : [1, 2, 0]\n",
      "data_index : 4\n",
      "data[data_index] : 195\n",
      "2\n",
      "targets_to_avoid : [1]\n",
      "2 0\n",
      "target : 0\n",
      "targets_to_avoid : [1, 0]\n",
      "2 1\n",
      "target : 2\n",
      "targets_to_avoid : [1, 0, 2]\n",
      "data_index : 5\n",
      "data[data_index] : 2\n",
      "3\n",
      "targets_to_avoid : [1]\n",
      "3 0\n",
      "target : 2\n",
      "targets_to_avoid : [1, 2]\n",
      "3 1\n",
      "target : 0\n",
      "targets_to_avoid : [1, 2, 0]\n",
      "data_index : 6\n",
      "data[data_index] : 3136\n"
     ]
    }
   ],
   "source": [
    "# for i in range(batch_size // num_skips):\n",
    "#     target = skip_window  # target label at the center of the buffer\n",
    "#     targets_to_avoid = [skip_window]\n",
    "#     for j in range(num_skips):\n",
    "#         while target in targets_to_avoid:\n",
    "#             target = random.randint(0, span - 1)\n",
    "#         targets_to_avoid.append(target)\n",
    "#         batch[i * num_skips + j] = buffer[skip_window]\n",
    "#         labels[i * num_skips + j, 0] = buffer[target]\n",
    "#     buffer.append(data[data_index])\n",
    "#     data_index = (data_index + 1) % len(data)\n",
    "\n",
    "for i in range(batch_size // num_skips):\n",
    "    print(i)\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    print (\"targets_to_avoid : {}\".format(targets_to_avoid))\n",
    "    for j in range(num_skips):\n",
    "        print (i, j)\n",
    "        while target in targets_to_avoid:\n",
    "            target = random.randint(0, span - 1)\n",
    "        print (\"target : {}\".format(target))\n",
    "        targets_to_avoid.append(target)\n",
    "        print (\"targets_to_avoid : {}\".format(targets_to_avoid))\n",
    "        batch[i * num_skips + j] = buffer[skip_window]\n",
    "        labels[i * num_skips + j, 0] = buffer[target]\n",
    "    print(\"data_index : {}\".format(data_index))\n",
    "    print(\"data[data_index] : {}\".format(data[data_index]))\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 195, 2, 3136)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3], data[4], data[5], data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([195, 2, 3136])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "data_index = (data_index + len(data) - span) % len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3083, 3083,   12,   12,    6,    6,  195,  195], dtype=int32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5244],\n",
       "       [  12],\n",
       "       [   6],\n",
       "       [3083],\n",
       "       [  12],\n",
       "       [ 195],\n",
       "       [   2],\n",
       "       [   6]], dtype=int32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결국 마지막에 generate_batch 함수는 batch, labels를 return 한다.\n",
    "\n",
    "와, 하나도 이해가 안되는데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3083 originated -> 5244 anarchism\n",
      "3083 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3083 originated\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],'->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('originated', 'anarchism')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[3083], reverse_dictionary[5244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5244, 3083, 12, 6, 195, 2, 3136]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아아!!  \n",
    "batch, labels에는 batch_size만큼 크기가 할당되어 있고,  \n",
    "for문 출력한걸 보자하니 skip_window만큼 앞 뒤 단어를 가져와서 일종의 쌍을 만들어주는 함수다.\n",
    "\n",
    "[5244, 3083, 12] 가 연결되어 있으니 3083 -> 5244, 3083 -> 12가 출력되어 있고 계속 그런식..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(batch_size // num_skips):\n",
    "#     target = skip_window  # target label at the center of the buffer\n",
    "#     targets_to_avoid = [skip_window]\n",
    "#     for j in range(num_skips):\n",
    "#         while target in targets_to_avoid:\n",
    "#             target = random.randint(0, span - 1)\n",
    "#         targets_to_avoid.append(target)\n",
    "#         batch[i * num_skips + j] = buffer[skip_window]\n",
    "#         labels[i * num_skips + j, 0] = buffer[target]\n",
    "#     buffer.append(data[data_index])\n",
    "#     data_index = (data_index + 1) % len(data)\n",
    "\n",
    "# 결국 밑에 2줄은 위에서 In[173] 과 같은 부분이며 역할은 일종의 초기화 작업을 해 준 것이다.\n",
    "# 파라미터는 batch_size = 8, num_skips = 2, skip_window = 1 이다.\n",
    "# targets_to_avoid에는 skip_window. 즉 1이 들어가있고 그 1은 자기 자신을 뜻한다.\n",
    "# j를 통한 for문을 통해 num_skips만큼 앞뒤의 단어를 가져와 batch에는 자신의 단어, labels에는 앞, 뒤 단어를 넣게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build and train a skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                        biases=nce_biases,\n",
    "                        labels=train_labels,\n",
    "                        inputs=embed,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.placeholder는 텐서플로우 자료형의 하나로 여러 학습 데이터를 답는 공간이다.\n",
    "\n",
    "조대협의 텐서플로우, 자료형의 이해 - http://bcho.tistory.com/1150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_uniform는 난수 생성기.  \n",
    "tf.nn.embedding_lookup의 nn은 뉴럴네트웍스인 것 같고 embedding_looup은 임베딩 계산을 위한 부분으로 보인다.  \n",
    "문서 (https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) 참조.\n",
    "\n",
    "nce_weights, nce_biases 둘 다 nce loss계산을 위한 변수인데  \n",
    "전부 reduce_mean, nce_loss에서 쓰인다. nce 손실함수를 사용하는 것이다.\n",
    "\n",
    "이에 대한 개념은 위에서도 언급했지만 http://solarisailab.com/archives/374 이 블로그가 도움이 되었다.\n",
    "\n",
    "SGD optimizer를 하는 부분에 결국 gradient descent에 관련된 부분인데 optimization 대한 이론은  \n",
    "http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html 여기에 잘 나와있다.\n",
    "\n",
    "마지막은 minibatch 데이터와 모든 임베딩간의 코사인 유사도를 구하는 부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Q. 코사인유사도가 아닌 다른 방법을 쓸 수 있을까? 유클리드나 ts-ss... **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print('Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  251.43145752\n",
      "Nearest to after: nonce, vintage, four, smile, encoding, powerplant, helga, silos,\n",
      "Nearest to first: hank, pearse, ut, kevin, sentries, ishaq, advisers, triads,\n",
      "Nearest to such: gomes, bertha, recounts, daytona, fishermen, clemens, jacobus, tested,\n",
      "Nearest to had: red, regicide, elo, bfi, kluwer, homelessness, nada, rob,\n",
      "Nearest to one: nuanced, homographs, dacia, rewriting, scarp, dialectical, medici, cider,\n",
      "Nearest to used: och, electromagnetic, replicated, gaming, manual, universiteit, reverence, wied,\n",
      "Nearest to which: klm, wynn, graziani, maidens, cmt, copernicus, diarrhoea, appellant,\n",
      "Nearest to world: chiles, magee, land, customers, kwahu, osaka, poudre, oui,\n",
      "Nearest to may: ascribes, clip, cheated, incurred, inductors, swordsmen, yoshi, intrepid,\n",
      "Nearest to no: redundantly, kilobit, ligature, gottfried, lexis, pfa, lowercase, brahmic,\n",
      "Nearest to american: goryeo, overseas, sara, leif, pesos, adad, vicious, saya,\n",
      "Nearest to about: ankobra, furniture, intercept, disapproved, discontent, calendars, availability, cop,\n",
      "Nearest to while: pan, buddhism, parliamentarism, tunny, motala, merian, gravitons, amalgamation,\n",
      "Nearest to an: proceeding, importer, philosopher, reflexive, schottenheimer, quaternions, andante, millie,\n",
      "Nearest to eight: belshazzar, aristotelian, jumpers, promptly, themis, tuple, spokes, seeds,\n",
      "Nearest to its: shing, qaida, dect, vim, centralized, gills, demyelinating, altars,\n",
      "Average loss at step  2000 :  114.403690899\n",
      "Average loss at step  4000 :  51.2888612986\n",
      "Average loss at step  6000 :  32.6488097588\n",
      "Average loss at step  8000 :  23.4513892732\n",
      "Average loss at step  10000 :  18.1649382942\n",
      "Nearest to after: four, of, and, cummings, encoding, exeter, pitch, episcopal,\n",
      "Nearest to first: early, wikisource, and, patience, argument, dozen, advisers, episcopal,\n",
      "Nearest to such: mary, j, jerk, molten, fishermen, ability, julia, limit,\n",
      "Nearest to had: bifurcation, episcopal, was, red, manufacturer, intersect, elo, encompassed,\n",
      "Nearest to one: two, nine, erlang, UNK, eggs, episcopal, zero, extension,\n",
      "Nearest to used: gif, electromagnetic, met, replicated, wikisource, episcopal, eggs, persons,\n",
      "Nearest to which: wikisource, that, eggs, battalion, tracks, salamander, water, result,\n",
      "Nearest to world: land, eggs, extraterrestrial, nicobar, gender, devised, postulate, wheels,\n",
      "Nearest to may: inductors, episcopal, to, battalion, question, incurred, oscillating, manner,\n",
      "Nearest to no: salamander, explained, infected, extension, davis, multimedia, lowercase, rest,\n",
      "Nearest to american: patience, overseas, termination, observation, idiom, exeter, internal, poole,\n",
      "Nearest to about: furniture, intercept, revealed, eggs, position, availability, specified, israeli,\n",
      "Nearest to while: exeter, wikisource, pan, nine, bigg, respected, steps, late,\n",
      "Nearest to an: reflexive, the, philosopher, derived, gif, lancashire, ransom, reserves,\n",
      "Nearest to eight: nine, zero, episcopal, battalion, wikisource, gif, aberration, bigg,\n",
      "Nearest to its: cl, wikisource, unsolicited, involvement, centralized, conspiracy, the, breed,\n",
      "Average loss at step  12000 :  14.2253790797\n",
      "Average loss at step  14000 :  11.616953439\n",
      "Average loss at step  16000 :  10.2889671261\n",
      "Average loss at step  18000 :  8.68207781422\n",
      "Average loss at step  20000 :  7.97439912355\n",
      "Nearest to after: of, four, and, equus, encoding, cummings, sky, with,\n",
      "Nearest to first: and, early, wikisource, fushimi, dozen, in, between, ut,\n",
      "Nearest to such: mary, equus, fishermen, assessed, julia, jerk, j, kiang,\n",
      "Nearest to had: was, is, bifurcation, has, encompassed, elo, regicide, are,\n",
      "Nearest to one: two, widehat, three, four, kiang, patience, eggs, episcopal,\n",
      "Nearest to used: och, replicated, electromagnetic, met, gif, eggs, wikisource, equatorial,\n",
      "Nearest to which: that, and, wikisource, eggs, battalion, this, also, psych,\n",
      "Nearest to world: extraterrestrial, kiang, criminal, nicobar, pel, eggs, gender, devised,\n",
      "Nearest to may: would, nine, to, must, can, rows, inductors, episcopal,\n",
      "Nearest to no: widehat, salamander, infected, gottfried, a, dinner, godwin, multimedia,\n",
      "Nearest to american: unmarried, patience, overseas, manage, termination, exeter, widehat, brings,\n",
      "Nearest to about: furniture, mandy, intercept, revealed, and, gprs, witchcraft, position,\n",
      "Nearest to while: exeter, wikisource, layla, pan, widehat, buddhism, instruments, respected,\n",
      "Nearest to an: the, baudot, reflexive, proceeding, philosopher, kiang, talkin, widehat,\n",
      "Nearest to eight: nine, zero, five, widehat, seven, six, episcopal, battalion,\n",
      "Nearest to its: the, widehat, his, wikisource, parthians, involvement, cl, phoca,\n",
      "Average loss at step  22000 :  7.20222205293\n",
      "Average loss at step  24000 :  6.88598056769\n",
      "Average loss at step  26000 :  6.61987834716\n",
      "Average loss at step  28000 :  6.2196433841\n",
      "Average loss at step  30000 :  6.67830052096\n",
      "Nearest to after: of, four, with, five, in, encoding, cummings, equus,\n",
      "Nearest to first: in, early, fushimi, taunus, wikisource, dozen, between, at,\n",
      "Nearest to such: mary, equus, fishermen, assessed, julia, well, feudalism, jerk,\n",
      "Nearest to had: was, has, have, encompassed, were, is, are, bifurcation,\n",
      "Nearest to one: two, taunus, three, zilog, eight, widehat, eggs, four,\n",
      "Nearest to used: replicated, electromagnetic, och, zilog, met, gif, eggs, netbios,\n",
      "Nearest to which: that, this, and, zilog, wikisource, eggs, also, mvs,\n",
      "Nearest to world: criminal, excommunication, extraterrestrial, exeter, kiang, and, oui, taunus,\n",
      "Nearest to may: would, can, must, to, should, nine, will, rows,\n",
      "Nearest to no: widehat, salamander, infected, a, six, nine, gottfried, pitts,\n",
      "Nearest to american: and, unmarried, exeter, manage, parody, poole, patience, termination,\n",
      "Nearest to about: furniture, intercept, mandy, communicated, corp, fealty, revealed, witchcraft,\n",
      "Nearest to while: exeter, layla, wikisource, pan, may, taunus, buddhism, for,\n",
      "Nearest to an: baudot, pitts, the, proceeding, reflexive, importer, kiang, philosopher,\n",
      "Nearest to eight: nine, five, seven, four, zero, six, three, modula,\n",
      "Nearest to its: the, his, widehat, their, mister, involvement, wikisource, parthians,\n",
      "Average loss at step  32000 :  5.9053851428\n",
      "Average loss at step  34000 :  5.68785658503\n",
      "Average loss at step  36000 :  5.71944525015\n",
      "Average loss at step  38000 :  5.51470567036\n",
      "Average loss at step  40000 :  5.03612590158\n",
      "Nearest to after: and, with, of, five, sibilant, in, four, glas,\n",
      "Nearest to first: stadtbahn, glas, rgb, fushimi, early, in, and, asynchronous,\n",
      "Nearest to such: mary, equus, well, fishermen, assessed, some, athlete, julia,\n",
      "Nearest to had: has, have, was, were, encompassed, is, are, bifurcation,\n",
      "Nearest to one: two, four, six, three, catfish, eight, glas, five,\n",
      "Nearest to used: replicated, danio, zilog, eggs, och, met, electromagnetic, netbios,\n",
      "Nearest to which: that, this, also, barb, catfish, zilog, glas, it,\n",
      "Nearest to world: criminal, danio, stadtbahn, excommunication, exeter, kiang, eggs, taunus,\n",
      "Nearest to may: can, would, must, will, should, could, while, to,\n",
      "Nearest to no: widehat, crystallography, a, infected, gps, salamander, six, licinius,\n",
      "Nearest to american: unmarried, and, glas, exeter, taunus, patience, manage, parody,\n",
      "Nearest to about: mandy, furniture, ankobra, intercept, catfish, corp, communicated, six,\n",
      "Nearest to while: glas, exeter, layla, and, wikisource, parliamentarism, may, when,\n",
      "Nearest to an: baudot, pitts, the, proceeding, reflexive, importer, kiang, catfish,\n",
      "Nearest to eight: nine, six, five, four, seven, three, zero, catfish,\n",
      "Nearest to its: his, their, the, widehat, catfish, mister, kiang, wikisource,\n",
      "Average loss at step  42000 :  5.4377039386\n",
      "Average loss at step  44000 :  5.44520079684\n",
      "Average loss at step  46000 :  5.34915550196\n",
      "Average loss at step  48000 :  5.31837329531\n",
      "Average loss at step  50000 :  5.16084573746\n",
      "Nearest to after: sibilant, when, with, in, five, since, of, glas,\n",
      "Nearest to first: stadtbahn, in, galatea, second, glas, fushimi, asynchronous, rgb,\n",
      "Nearest to such: equus, well, mary, some, assessed, these, fishermen, many,\n",
      "Nearest to had: has, have, was, were, encompassed, galatea, battalion, are,\n",
      "Nearest to one: two, four, six, seven, five, three, eight, catfish,\n",
      "Nearest to used: replicated, zilog, danio, eggs, och, wikisource, met, catfish,\n",
      "Nearest to which: that, this, and, catfish, barb, it, glas, zilog,\n",
      "Nearest to world: criminal, grue, danio, excommunication, exeter, stadtbahn, oui, surfaces,\n",
      "Nearest to may: can, would, must, could, will, should, while, nine,\n",
      "Nearest to no: widehat, a, crystallography, infected, gps, salamander, licinius, it,\n",
      "Nearest to american: unmarried, glas, exeter, whipple, english, grapevines, internal, manage,\n",
      "Nearest to about: mandy, furniture, communicated, intercept, ankobra, corp, catfish, attacking,\n",
      "Nearest to while: when, glas, exeter, galatea, layla, wikisource, catfish, and,\n",
      "Nearest to an: pitts, baudot, proceeding, the, importer, talkin, klondike, reflexive,\n",
      "Nearest to eight: nine, seven, six, five, four, three, zero, catfish,\n",
      "Nearest to its: their, the, his, catfish, widehat, mister, kiang, wikisource,\n",
      "Average loss at step  52000 :  5.16963072491\n",
      "Average loss at step  54000 :  4.97174452293\n",
      "Average loss at step  56000 :  4.92610533047\n",
      "Average loss at step  58000 :  5.05796637619\n",
      "Average loss at step  60000 :  4.92862518322\n",
      "Nearest to after: when, before, in, since, sibilant, five, with, of,\n",
      "Nearest to first: stadtbahn, second, galatea, fushimi, asynchronous, glas, osr, wikisource,\n",
      "Nearest to such: well, some, these, equus, mary, many, assessed, fishermen,\n",
      "Nearest to had: has, have, was, were, encompassed, battalion, galatea, guti,\n",
      "Nearest to one: two, four, seven, six, eight, five, three, catfish,\n",
      "Nearest to used: replicated, zilog, danio, eggs, och, hunan, wikisource, similar,\n",
      "Nearest to which: that, this, it, also, and, barb, there, catfish,\n",
      "Nearest to world: criminal, grue, exeter, stadtbahn, excommunication, danio, surfaces, oui,\n",
      "Nearest to may: can, would, must, could, will, should, cannot, while,\n",
      "Nearest to no: a, widehat, crystallography, infected, gps, salamander, licinius, miniszt,\n",
      "Nearest to american: unmarried, english, exeter, glas, whipple, grapevines, internal, absorptive,\n",
      "Nearest to about: mandy, communicated, furniture, intercept, corp, six, catfish, hangover,\n",
      "Nearest to while: when, but, glas, galatea, and, exeter, layla, however,\n",
      "Nearest to an: pitts, importer, baudot, proceeding, klondike, talkin, the, reflexive,\n",
      "Nearest to eight: nine, six, seven, four, five, three, zero, catfish,\n",
      "Nearest to its: their, his, the, catfish, widehat, kiang, mister, wikisource,\n",
      "Average loss at step  62000 :  4.84687075353\n",
      "Average loss at step  64000 :  5.08169235206\n",
      "Average loss at step  66000 :  4.89735970485\n",
      "Average loss at step  68000 :  4.83465363526\n",
      "Average loss at step  70000 :  4.88326957357\n",
      "Nearest to after: when, before, since, during, sibilant, with, in, while,\n",
      "Nearest to first: second, stadtbahn, asynchronous, galatea, landesverband, fushimi, osr, glas,\n",
      "Nearest to such: well, some, these, many, equus, mary, assessed, known,\n",
      "Nearest to had: has, have, was, were, battalion, encompassed, galatea, judas,\n",
      "Nearest to one: two, six, seven, three, four, hyi, five, catfish,\n",
      "Nearest to used: replicated, zilog, eggs, danio, och, sloti, wikisource, hunan,\n",
      "Nearest to which: that, this, it, also, barb, catfish, zilog, there,\n",
      "Nearest to world: criminal, stadtbahn, grue, danio, exeter, bbs, ebne, surfaces,\n",
      "Nearest to may: can, would, must, will, could, should, cannot, might,\n",
      "Nearest to no: a, widehat, infected, crystallography, gps, salamander, it, licinius,\n",
      "Nearest to american: unmarried, english, exeter, absorptive, whipple, internal, grapevines, glas,\n",
      "Nearest to about: mandy, communicated, furniture, corp, intercept, six, catfish, hangover,\n",
      "Nearest to while: when, but, hyi, landesverband, glas, galatea, however, or,\n",
      "Nearest to an: pitts, importer, proceeding, baudot, klondike, talkin, landesverband, the,\n",
      "Nearest to eight: seven, nine, six, five, four, three, zero, two,\n",
      "Nearest to its: their, his, the, hyi, widehat, catfish, kiang, mister,\n",
      "Average loss at step  72000 :  4.83956688797\n",
      "Average loss at step  74000 :  4.68058289599\n",
      "Average loss at step  76000 :  4.66559380651\n",
      "Average loss at step  78000 :  4.75288369477\n",
      "Average loss at step  80000 :  4.70548305917\n",
      "Nearest to after: before, when, since, during, in, while, sibilant, with,\n",
      "Nearest to first: second, stadtbahn, yaum, asynchronous, landesverband, galatea, fushimi, last,\n",
      "Nearest to such: these, well, many, some, equus, known, mary, chevrolet,\n",
      "Nearest to had: has, have, was, were, battalion, encompassed, judas, widehat,\n",
      "Nearest to one: two, seven, hyi, five, three, six, catfish, four,\n",
      "Nearest to used: replicated, zilog, eggs, sloti, danio, wikisource, och, known,\n",
      "Nearest to which: that, this, it, also, but, bde, there, and,\n",
      "Nearest to world: risotto, criminal, stadtbahn, grue, ebne, exeter, danio, antiquark,\n",
      "Nearest to may: can, would, must, will, could, should, cannot, might,\n",
      "Nearest to no: a, widehat, infected, crystallography, gps, now, hyi, salamander,\n",
      "Nearest to american: unmarried, english, british, exeter, grapevines, glas, impulses, internal,\n",
      "Nearest to about: mandy, communicated, furniture, corp, intercept, attacking, six, esas,\n",
      "Nearest to while: when, or, but, however, landesverband, hyi, and, galatea,\n",
      "Nearest to an: pitts, importer, proceeding, the, baudot, talkin, landesverband, klondike,\n",
      "Nearest to eight: seven, six, four, five, nine, zero, three, widehat,\n",
      "Nearest to its: their, his, the, hyi, widehat, catfish, her, mister,\n",
      "Average loss at step  82000 :  4.73124564517\n",
      "Average loss at step  84000 :  4.70410366321\n",
      "Average loss at step  86000 :  4.66687153327\n",
      "Average loss at step  88000 :  4.59802723742\n",
      "Average loss at step  90000 :  4.30602305174\n",
      "Nearest to after: before, when, since, during, while, with, sibilant, in,\n",
      "Nearest to first: second, stadtbahn, yaum, last, asynchronous, landesverband, next, galatea,\n",
      "Nearest to such: these, well, many, some, known, equus, chevrolet, assessed,\n",
      "Nearest to had: has, have, was, were, is, encompassed, battalion, judas,\n",
      "Nearest to one: hyi, seven, catfish, two, bde, six, five, landesverband,\n",
      "Nearest to used: replicated, zilog, sloti, danio, eggs, known, wikisource, hunan,\n",
      "Nearest to which: that, this, it, also, there, but, catfish, bde,\n",
      "Nearest to world: risotto, criminal, ebne, stadtbahn, exeter, grue, antiquark, danio,\n",
      "Nearest to may: can, would, must, will, could, should, cannot, might,\n",
      "Nearest to no: widehat, infected, nine, a, crystallography, gps, hyi, now,\n",
      "Nearest to american: and, english, british, cherry, whipple, exeter, glas, classed,\n",
      "Nearest to about: mandy, communicated, corp, furniture, libi, esas, attacking, simpsons,\n",
      "Nearest to while: when, however, but, landesverband, though, hyi, or, galatea,\n",
      "Nearest to an: proceeding, importer, pitts, baudot, klondike, talkin, landesverband, eolian,\n",
      "Nearest to eight: seven, six, five, nine, four, three, zero, two,\n",
      "Nearest to its: their, his, the, her, hyi, widehat, catfish, mister,\n",
      "Average loss at step  92000 :  4.4847347492\n",
      "Average loss at step  94000 :  4.46486251056\n",
      "Average loss at step  96000 :  4.52238334382\n",
      "Average loss at step  98000 :  4.57712128961\n",
      "Average loss at step  100000 :  4.58160655975\n",
      "Nearest to after: before, when, during, since, while, in, five, sibilant,\n",
      "Nearest to first: second, stadtbahn, last, yaum, fourth, next, landesverband, arctocephalus,\n",
      "Nearest to such: these, many, well, some, known, equus, chevrolet, investigations,\n",
      "Nearest to had: has, have, was, were, battalion, encompassed, been, is,\n",
      "Nearest to one: two, hyi, three, six, four, seven, catfish, bde,\n",
      "Nearest to used: replicated, known, sloti, zilog, danio, hunan, epistle, och,\n",
      "Nearest to which: that, this, also, it, and, but, there, one,\n",
      "Nearest to world: risotto, criminal, ebne, stadtbahn, exeter, danio, antiquark, grue,\n",
      "Nearest to may: can, would, must, could, will, should, cannot, might,\n",
      "Nearest to no: cowdery, widehat, infected, crystallography, a, gps, nine, gerardo,\n",
      "Nearest to american: english, british, french, german, cherry, classed, whipple, unmarried,\n",
      "Nearest to about: mandy, communicated, corp, libi, simpsons, attacking, furniture, esas,\n",
      "Nearest to while: when, but, however, though, and, although, hyi, landesverband,\n",
      "Nearest to an: proceeding, importer, pitts, klondike, baudot, the, landesverband, talkin,\n",
      "Nearest to eight: seven, six, nine, four, five, three, zero, landesverband,\n",
      "Nearest to its: their, his, the, her, widehat, hyi, catfish, mister,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize the embeddings.\n",
    "\n",
    "6은 패스...  \n",
    "아직 내가 당장 필요하지는 않다.\n",
    "\n",
    "### 후기\n",
    "\n",
    "솔직히 어렵다... 아무것도 없는 상태에서 이런걸 짜라고 하면 짤 수 있을까?...  \n",
    "자신이 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
